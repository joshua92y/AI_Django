# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import os
import pickle
import torch
from dotenv import load_dotenv

from langchain.document_loaders import UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Qdrant
from qdrant_client import QdrantClient

from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain

# âœ… 0. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env ëŒ€ì‹  env.txt ì‚¬ìš©)
print('âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ')
env_path = '/content/env.txt'
load_dotenv(dotenv_path=env_path, override=True)
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# âœ… 1. ë””ë°”ì´ìŠ¤ ì„¤ì • (Colab GPU ì‚¬ìš© ìµœì í™”)
print('âœ… Colab GPU ìµœì í™” ì„¤ì •')
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… 2. PDF â†’ ë¬¸ì¥ ë¶„í•  + í˜ì´ì§€ ë²”ìœ„ ì¶”ì • + ìºì‹±
print('âœ… PDF ë¬¸ì„œ ì²˜ë¦¬')
pdf_path = "/content/drive/MyDrive/data/á„’á…¡á†«á„’á…ªá„‰á…¢á†¼á„†á…§á†¼ á„Œá…µá†«á„‰á…µá†·á„€á…¡á„ƒá…³á†¨Há„‡á…©á„Œá…¡á†¼á„‡á…©á„’á…¥á†· á„†á…®á„‡á…¢á„ƒá…¡á†¼_2126-A01~A03_á„‹á…£á†¨á„€á…ªá†«_20250501~.pdf"
output_path = "/content/drive/MyDrive/data/split_docs.pkl"

if os.path.exists(output_path):
    print("âœ… split_docs ìºì‹œ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    with open(output_path, "rb") as f:
        split_docs = pickle.load(f)
else:
    print("ğŸ“„ ìºì‹œ ì—†ìŒ â†’ ë¬¸ì„œ íŒŒì‹± ì‹œì‘...")
    loader = UnstructuredPDFLoader(pdf_path, mode="elements")
    pages = loader.load()

    # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ë§¤í•‘
    page_map = [{"text": doc.page_content, "page": idx} for idx, doc in enumerate(pages, start=1)]

    # ë¬¸ì¥ ë¶„í• 
    all_text = "\n".join([p["text"] for p in page_map])
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_texts = splitter.split_text(all_text)

    # í˜ì´ì§€ ë²”ìœ„ ì¶”ì • í•¨ìˆ˜
    def find_page_range(text_chunk, page_map):
        page_nums = []
        for p in page_map:
            if text_chunk in p["text"]:
                return str(p["page"])
            for line in text_chunk.split("\n"):
                if line.strip() and line.strip() in p["text"]:
                    page_nums.append(p["page"])
                    break
        if not page_nums:
            return "unknown"
        elif len(set(page_nums)) == 1:
            return str(page_nums[0])
        else:
            return f"{min(page_nums)}-{max(page_nums)}"

    # ë¬¸ì„œ ê°ì²´ êµ¬ì„±
    split_docs = [
        Document(page_content=chunk, metadata={"page_range": find_page_range(chunk, page_map)})
        for chunk in split_texts
    ]

    # ì €ì¥
    with open(output_path, "wb") as f:
        pickle.dump(split_docs, f)

    print(f"âœ… ì´ {len(split_docs)}ê°œì˜ ë¬¸ì„œ ì €ì¥ë¨ â†’ {output_path}")
    print("ğŸ” ì²« ë¬¸ì„œ ì˜ˆì‹œ:", split_docs[0].metadata, "\n", split_docs[0].page_content[:200])

# âœ… 3. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
print('âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (BGE-M3 Korean)')
embedding_model = HuggingFaceEmbeddings(
    model_name="upskyy/bge-m3-korean",
    model_kwargs={"device": device}
)

# âœ… 4. Qdrant ì—…ë¡œë“œ or ì¬ì‚¬ìš©
collection_name = "insurance_docs"
client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

if client.collection_exists(collection_name):
    print(f"âœ… Qdrant ì»¬ë ‰ì…˜ '{collection_name}' ì¬ì‚¬ìš© ì¤‘")
    vectorstore = Qdrant(
        collection_name=collection_name,
        url=QDRANT_URL,
        api_key=QDRANT_API_KEY,
        embedding_function=embedding_model,
        prefer_grpc=False
    )
else:
    print(f"ğŸ“¦ Qdrant ì»¬ë ‰ì…˜ '{collection_name}' ì—†ìŒ â†’ ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘...")
    vectorstore = Qdrant.from_documents(
        documents=split_docs,
        embedding=embedding_model,
        url=QDRANT_URL,
        api_key=QDRANT_API_KEY,
        collection_name=collection_name,
        prefer_grpc=False
    )
    print(f"âœ… ì´ {len(split_docs)}ê°œ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ!")

# âœ… 5. ëŒ€í™”í˜• QA êµ¬ì„±

# ë©”ëª¨ë¦¬ (ë¬¸ë§¥ ìœ ì§€)
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# í”„ë¡¬í”„íŠ¸ ì„¤ì •
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "ë‹¹ì‹ ì€ ë˜‘ë˜‘í•œ AI ë³´í—˜ì„¤ê³„ì‚¬ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ ì¡°í•­ê³¼ í‘œë¥¼ í•¨ê»˜ ê³ ë ¤í•´ì„œ ë…¼ë¦¬ì ì¸ ì¶”ë¡ ì„ ê±°ì³ ì •í™•íˆ ë‹µí•˜ì„¸ìš”.\n
í‘œ ë‚´ìš©ì€ êµ¬ì¡°ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , í•´ë‹¹ ê¸ˆì•¡ì´ë‚˜ ì¡°ê±´ì„ ëª…í™•íˆ ì „ë‹¬í•´ ì£¼ì„¸ìš”.\n"
        "ì°¸ê³ : {context}\n"
        "ì§ˆë¬¸: {question}\n"
        "ë‹µë³€:"
    )
)

# Retriever êµ¬ì„±
retriever = vectorstore.as_retriever()

# Conversational QA ì²´ì¸ êµ¬ì„±
print('# âœ… Conversational Retrieval QA ì²´ì¸ êµ¬ì„±')
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(model="gpt-4o-mini", api_key=OPENAI_API_KEY),
    retriever=retriever,
    memory=memory,
    combine_docs_chain_kwargs={"prompt": prompt},
    return_source_documents=True
)

# âœ… 6. ì§ˆì˜ ì˜ˆì‹œ
query = "ì¬í•´ë¡œ ì¸í•´ ì¥í•´ê°€ ìƒê²¼ì–´ ì–¼ë§ˆ ë°›ì„ ìˆ˜ ìˆì–´?"
print('ğŸ’¬ ì§ˆì˜:', query)
result = qa_chain({"question": query})

# âœ… 7. ê²°ê³¼ í™•ì¸
print('ğŸ“š ê´€ë ¨ ë¬¸ì„œ:', result["source_documents"][0].metadata)
print("âœ… ë‹µë³€:", result["answer"])

# âœ… (ì„ íƒ) retriever ì§ì ‘ í™•ì¸
docs = retriever.get_relevant_documents(query)
print(f"ğŸ” ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì€ ë¬¸ì„œ ìˆ˜: {len(docs)}")
print("ğŸ“„ ì²« ë¬¸ì„œ preview:", docs[0].metadata, "\n", docs[0].page_content[:200])
